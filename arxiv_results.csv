title,authors,published,updated,summary,pdf_url,arxiv_id,categories,doi,journal_ref,comment
Probabilistic Inferences in Bayesian Networks,Jianguo Ding,2010-11-03T16:50:22Z,2010-11-05T14:14:33Z,"Bayesian network is a complete model for the variables and their
relationships, it can be used to answer probabilistic queries about them. A
Bayesian network can thus be considered a mechanism for automatically applying
Bayes' theorem to complex problems. In the application of Bayesian networks,
most of the work is related to probabilistic inferences. Any variable updating
in any node of Bayesian networks might result in the evidence propagation
across the Bayesian networks. This paper sums up various inference techniques
in Bayesian networks and provide guidance for the algorithm calculation in
probabilistic inference in Bayesian networks.",http://arxiv.org/pdf/1011.0935v2,1011.0935v2,"['cs.AI', 'cs.NI']",,,"14 pages, 9 figures, book chapter"
Semiparametric Bayesian Networks,"David Atienza, Concha Bielza, Pedro Larrañaga",2021-09-07T11:47:32Z,2021-09-07T11:47:32Z,"We introduce semiparametric Bayesian networks that combine parametric and
nonparametric conditional probability distributions. Their aim is to
incorporate the advantages of both components: the bounded complexity of
parametric models and the flexibility of nonparametric ones. We demonstrate
that semiparametric Bayesian networks generalize two well-known types of
Bayesian networks: Gaussian Bayesian networks and kernel density estimation
Bayesian networks. For this purpose, we consider two different conditional
probability distributions required in a semiparametric Bayesian network. In
addition, we present modifications of two well-known algorithms (greedy
hill-climbing and PC) to learn the structure of a semiparametric Bayesian
network from data. To realize this, we employ a score function based on
cross-validation. In addition, using a validation dataset, we apply an
early-stopping criterion to avoid overfitting. To evaluate the applicability of
the proposed algorithm, we conduct an exhaustive experiment on synthetic data
sampled by mixing linear and nonlinear functions, multivariate normal data
sampled from Gaussian Bayesian networks, real data from the UCI repository, and
bearings degradation data. As a result of this experiment, we conclude that the
proposed algorithm accurately learns the combination of parametric and
nonparametric components, while achieving a performance comparable with those
provided by state-of-the-art methods.",http://arxiv.org/pdf/2109.03008v1,2109.03008v1,"['cs.LG', 'stat.ML', '68T05 68T10', 'I.5.1; I.2.6']",,,"44 pages, 13 figures, 4 tables, submitted to Information Sciences"
Are Bayesian networks typically faithful?,"Philip Boeken, Patrick Forré, Joris M. Mooij",2024-10-21T13:38:04Z,2025-01-20T09:40:00Z,"Faithfulness is a ubiquitous assumption in causal inference, often motivated
by the fact that the faithful parameters of linear Gaussian and discrete
Bayesian networks are typical, and the folklore belief that this should also
hold for other classes of Bayesian networks. We address this open question by
showing that among all Bayesian networks over a given DAG, the faithful
Bayesian networks are indeed `typical': they constitute a dense, open set with
respect to the total variation metric. However, this does not imply that
faithfulness is typical in restricted classes of Bayesian networks, as are
often considered in statistical applications. To this end we consider the class
of Bayesian networks parametrised by conditional exponential families, for
which we show that under mild regularity conditions, the faithful parameters
constitute a dense, open set and the unfaithful parameters have Lebesgue
measure zero, extending the existing results for linear Gaussian and discrete
Bayesian networks. Finally, we show that the aforementioned results also hold
for Bayesian networks with latent variables.",http://arxiv.org/pdf/2410.16004v2,2410.16004v2,"['math.ST', 'math.PR', 'stat.ML', 'stat.TH']",,,
Markov Equivalence of Max-Linear Bayesian Networks,"Carlos Améndola, Ben Hollering, Seth Sullivant, Ngoc Tran",2021-06-15T17:27:21Z,2021-06-15T17:27:21Z,"Max-linear Bayesian networks have emerged as highly applicable models for
causal inference via extreme value data. However, conditional independence (CI)
for max-linear Bayesian networks behaves differently than for classical
Gaussian Bayesian networks. We establish the parallel between the two theories
via tropicalization, and establish the surprising result that the Markov
equivalence classes for max-linear Bayesian networks coincide with the ones
obtained by regular CI. Our paper opens up many problems at the intersection of
extreme value statistics, causal inference and tropical geometry.",http://arxiv.org/pdf/2106.08305v1,2106.08305v1,"['math.ST', 'math.AG', 'math.CO', 'stat.TH', '62H22, 14T90, 62G32, 62R01']",,,"19 pages, 5 figures, accepted for the 37th conference on Uncertainty
  in Artificial Intelligence (UAI 2021)"
"Improving Cancer Imaging Diagnosis with Bayesian Networks and Deep
  Learning: A Bayesian Deep Learning Approach","Pei Xi, Lin",2024-03-28T01:27:10Z,2024-03-28T01:27:10Z,"With recent advancements in the development of artificial intelligence
applications using theories and algorithms in machine learning, many accurate
models can be created to train and predict on given datasets. With the
realization of the importance of imaging interpretation in cancer diagnosis,
this article aims to investigate the theory behind Deep Learning and Bayesian
Network prediction models. Based on the advantages and drawbacks of each model,
different approaches will be used to construct a Bayesian Deep Learning Model,
combining the strengths while minimizing the weaknesses. Finally, the
applications and accuracy of the resulting Bayesian Deep Learning approach in
the health industry in classifying images will be analyzed.",http://arxiv.org/pdf/2403.19083v1,2403.19083v1,"['cs.LG', 'cs.AI', 'eess.IV']",,,
Bayesian Networks for Brain-Computer Interfaces: A Survey,Pingsheng Li,2022-05-24T00:35:53Z,2022-05-24T00:35:53Z,"Brain-Computer Interface (BCI) is a rapidly developing technology that allows
direct communications between the human brain and external devices, such as
robotic arms and computers. Bayesian Networks is a powerful tool in machine
learning for tackling with problems that requires understanding and modelling
the uncertainty and complexity within complex system built by sub-modular
components. Therefore, deploying Bayesian Networks in the application of
Brain-Computer Interfaces becomes an increasingly popular approach in BCI
research. This survey covers related existing works in relatively high-level
perspectives, classifies the models and algorithms involved, and also
summarizes the application of Bayesian Networks or its variants in the context
of Brain-Computer Interfaces.",http://arxiv.org/pdf/2206.07487v1,2206.07487v1,['eess.SP'],,,
"Constrained Bayesian Networks: Theory, Optimization, and Applications","Paul Beaumont, Michael Huth",2017-05-15T16:48:12Z,2017-05-15T16:48:12Z,"We develop the theory and practice of an approach to modelling and
probabilistic inference in causal networks that is suitable when
application-specific or analysis-specific constraints should inform such
inference or when little or no data for the learning of causal network
structure or probability values at nodes are available. Constrained Bayesian
Networks generalize a Bayesian Network such that probabilities can be symbolic,
arithmetic expressions and where the meaning of the network is constrained by
finitely many formulas from the theory of the reals. A formal semantics for
constrained Bayesian Networks over first-order logic of the reals is given,
which enables non-linear and non-convex optimisation algorithms that rely on
decision procedures for this logic, and supports the composition of several
constrained Bayesian Networks. A non-trivial case study in arms control, where
few or no data are available to assess the effectiveness of an arms inspection
process, evaluates our approach. An open-access prototype implementation of
these foundations and their algorithms uses the SMT solver Z3 as decision
procedure, leverages an open-source package for Bayesian inference to symbolic
computation, and is evaluated experimentally.",http://arxiv.org/pdf/1705.05326v1,1705.05326v1,['cs.AI'],,,"43 pages, 18 figures"
On tensor rank of conditional probability tables in Bayesian networks,"Jiří Vomlel, Petr Tichavský",2014-09-22T19:32:15Z,2014-09-22T19:32:15Z,"A difficult task in modeling with Bayesian networks is the elicitation of
numerical parameters of Bayesian networks. A large number of parameters is
needed to specify a conditional probability table (CPT) that has a larger
parent set. In this paper we show that, most CPTs from real applications of
Bayesian networks can actually be very well approximated by tables that require
substantially less parameters. This observation has practical consequence not
only for model elicitation but also for efficient probabilistic reasoning with
these networks.",http://arxiv.org/pdf/1409.6287v1,1409.6287v1,"['cs.AI', '68T37', 'I.2.3']",,,
"Bayesian Low-Rank LeArning (Bella): A Practical Approach to Bayesian
  Neural Networks","Bao Gia Doan, Afshar Shamsi, Xiao-Yu Guo, Arash Mohammadi, Hamid Alinejad-Rokny, Dino Sejdinovic, Damien Teney, Damith C. Ranasinghe, Ehsan Abbasnejad",2024-07-30T15:07:13Z,2025-01-16T14:45:36Z,"Computational complexity of Bayesian learning is impeding its adoption in
practical, large-scale tasks. Despite demonstrations of significant merits such
as improved robustness and resilience to unseen or out-of-distribution inputs
over their non- Bayesian counterparts, their practical use has faded to near
insignificance. In this study, we introduce an innovative framework to mitigate
the computational burden of Bayesian neural networks (BNNs). Our approach
follows the principle of Bayesian techniques based on deep ensembles, but
significantly reduces their cost via multiple low-rank perturbations of
parameters arising from a pre-trained neural network. Both vanilla version of
ensembles as well as more sophisticated schemes such as Bayesian learning with
Stein Variational Gradient Descent (SVGD), previously deemed impractical for
large models, can be seamlessly implemented within the proposed framework,
called Bayesian Low-Rank LeArning (Bella). In a nutshell, i) Bella achieves a
dramatic reduction in the number of trainable parameters required to
approximate a Bayesian posterior; and ii) it not only maintains, but in some
instances, surpasses the performance of conventional Bayesian learning methods
and non-Bayesian baselines. Our results with large-scale tasks such as
ImageNet, CAMELYON17, DomainNet, VQA with CLIP, LLaVA demonstrate the
effectiveness and versatility of Bella in building highly scalable and
practical Bayesian deep models for real-world applications.",http://arxiv.org/pdf/2407.20891v4,2407.20891v4,"['cs.LG', 'cs.AI', 'cs.CV']",,,This paper is accepted in AAAI'2025
Bayesian Network Models for Incomplete and Dynamic Data,Marco Scutari,2019-06-15T09:59:04Z,2019-10-24T12:17:33Z,"Bayesian networks are a versatile and powerful tool to model complex
phenomena and the interplay of their components in a probabilistically
principled way. Moving beyond the comparatively simple case of completely
observed, static data, which has received the most attention in the literature,
in this paper we will review how Bayesian networks can model dynamic data and
data with incomplete observations. Such data are the norm at the forefront of
research and in practical applications, and Bayesian networks are uniquely
positioned to model them due to their explainability and interpretability.",http://arxiv.org/pdf/1906.06513v2,1906.06513v2,"['stat.ME', 'math.ST', 'stat.ML', 'stat.TH']",,"Statistica Neerlandica (2020), 74(3), 397-419","24 pages, 4 figures"
